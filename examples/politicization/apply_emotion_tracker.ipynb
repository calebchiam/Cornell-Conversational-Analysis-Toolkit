{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Emotion Tracker\n",
    "Nov 14th 2019\n",
    "\n",
    "Testing that the emotion tracker works for utterances in corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required modules and set up environment\n",
    "import os\n",
    "\n",
    "# replace file path below with your own local convokit\n",
    "os.chdir('/Users/marianneaubin/Documents/Classes/CS6742/cs6742-fork')\n",
    "\n",
    "import convokit\n",
    "\n",
    "from convokit import Corpus, Parser, EmoTracker, Transformer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "corpus = convokit.Corpus(filename='../datasets/democrats-filtered-labelled-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 1702\n",
      "Number of Utterances: 5538\n",
      "Number of Conversations: 677\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed  500 utterances \n",
      "processed  1000 utterances \n",
      "processed  1500 utterances \n",
      "processed  2000 utterances \n",
      "processed  2500 utterances \n",
      "processed  3000 utterances \n",
      "processed  3500 utterances \n",
      "processed  4000 utterances \n",
      "processed  4500 utterances \n",
      "processed  5000 utterances \n",
      "processed  5500 utterances \n"
     ]
    }
   ],
   "source": [
    "et = EmoTracker();\n",
    "corpus = et.transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"emotional\", \"sadness\", \"negative_emotion\", \"shame\", \n",
    "              \"violence\", \"rage\", \"pain\", \"anger\", \"disgust\", \"hate\", \n",
    "                \"love\", \"politics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'emotional': 8, 'sadness': 9, 'negative_emotion': 17, 'shame': 3, 'violence': 13, 'rage': 3, 'pain': 10, 'anger': 2, 'disgust': 2, 'hate': 12, 'love': 7, 'politics': 9}\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "emotional_dist_dict = {\"emotional\": 0,\n",
    "                       \"sadness\": 0, \n",
    "                       \"negative_emotion\": 0,\n",
    "                       \"shame\": 0,\n",
    "                       \"violence\":0,\n",
    "                       \"rage\":0,\n",
    "                       \"pain\":0,\n",
    "                       \"anger\":0,\n",
    "                       \"disgust\":0,\n",
    "                       \"hate\":0, \n",
    "                       \"love\":0,\n",
    "                       \"politics\":0}\n",
    "for conv_id in corpus.conversations:\n",
    "    conv = corpus.get_conversation(conv_id)\n",
    "    for utt in conv.iter_utterances():\n",
    "        if utt.meta[\"analysis\"] != None:\n",
    "            for cat in categories:\n",
    "                if (utt.meta[\"analysis\"][cat] != 0.0):\n",
    "                    emotional_dist_dict[cat] = emotional_dist_dict[cat] + 1\n",
    "            counter = counter + 1\n",
    "print(emotional_dist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_ids = corpus.get_utterance_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utter_ids[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_word_counter = 0\n",
    "total_word_counter = 0\n",
    "counter = 0\n",
    "for x in range(0,3237456):\n",
    "    utt = corpus.get_utterance(utter_ids[x])\n",
    "    pol_word_counter = pol_word_counter + utt.meta['num_pol_refs']\n",
    "    total_word_counter = total_word_counter + utt.meta['num_pol_refs_incidence']\n",
    "    counter = counter + 1\n",
    "    if counter % 100000 == 0:\n",
    "        print(counter, \" completed\")\n",
    "    \n",
    "print(\"total # of words:\", pol_word_counter)\n",
    "print(total_word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "pol_words = {}\n",
    "for x in range(0, 3237456):\n",
    "    utt = corpus.get_utterance(utter_ids[x])\n",
    "    utt_pol_words = utt.meta['pol_words']\n",
    "    for y in utt_pol_words:\n",
    "        if (y not in pol_words.keys()):\n",
    "            pol_words[y] = 1\n",
    "        else:\n",
    "            pol_words[y] = pol_words[y] + 1\n",
    "    counter = counter + 1\n",
    "    if counter % 100000 == 0:\n",
    "        print(counter, \" completed\")\n",
    "        \n",
    "print(pol_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = []\n",
    "large_pol_words = []\n",
    "for x in pol_words.keys():\n",
    "    if pol_words[x] > 5000:\n",
    "        freqs.append(pol_words[x])\n",
    "        large_pol_words.append(x)\n",
    "        \n",
    "freqs, large_pol_words = (list(x) for x in zip(*sorted(zip(freqs, large_pol_words))))\n",
    "freqs.reverse()\n",
    "large_pol_words.reverse()\n",
    "print(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objects = large_pol_words\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.bar(y_pos, freqs, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most commonly used political words in Corpus')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "## ISSUE: right now, this only works for single words. \n",
    "## will need to change the transformer so that it can also\n",
    "## compute multiple words e.g. \"bill of rights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read in csv of relevant utts for sandy hook\n",
    "\n",
    "import csv\n",
    "#with open('../sandyhook_utterance_ids.csv', 'r') as f:\n",
    "with open('../auroratheater_utterance_ids.csv', 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    sh_list = list(reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## recompute political freqs\n",
    "\n",
    "pol_words_sh = {}\n",
    "for x in sh_list:\n",
    "    utt = corpus.get_utterance(x[0])\n",
    "    utt_pol_words = utt.meta['pol_words']\n",
    "    for y in utt_pol_words:\n",
    "        if (y not in pol_words_sh.keys()):\n",
    "            pol_words_sh[y] = 1\n",
    "        else:\n",
    "            pol_words_sh[y] = pol_words_sh[y] + 1\n",
    "        \n",
    "print(pol_words_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_sh = []\n",
    "large_pol_words_sh = []\n",
    "for x in pol_words_sh.keys():\n",
    "    freqs_sh.append(pol_words_sh[x])\n",
    "    large_pol_words_sh.append(x)\n",
    "        \n",
    "freqs_sh, large_pol_words_sh = (list(x) for x in zip(*sorted(zip(freqs_sh, large_pol_words_sh))))\n",
    "freqs_sh.reverse()\n",
    "large_pol_words_sh.reverse()\n",
    "print(freqs_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = large_pol_words_sh\n",
    "y_pos = np.arange(len(objects))\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(y_pos, freqs_sh, align='center', alpha=0.5)\n",
    "plt.xticks(y_pos, objects, rotation='vertical')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Most commonly used political words in Sandy Hook Corpus')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus.get_utterance(utter_ids[60]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandy Hook analysis\n",
    "Now we try to establish a time series of how many words there are per day after December 14, 2012 (Sandy Hook shooting day). Timestamp: 1355461200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import pandas as pd \n",
    "\n",
    "ps = 7\n",
    "\n",
    "#Sandy hook dates\n",
    "#start_date = '2012-12-14'\n",
    "#end_date = '2012-12-22'\n",
    "start_date = '2012-07-20'\n",
    "end_date = '2012-07-28'\n",
    "\n",
    "num_posts_sh = [0] * ps\n",
    "times = pd.date_range(start=start_date,end=end_date,periods=ps+1)\n",
    "times = np.array(times)\n",
    "bin_times = times[:-1]\n",
    "\n",
    "# convert to datetime object\n",
    "times_temp = []\n",
    "for i,x in enumerate(times):\n",
    "    times_temp.append(pd.to_datetime(x))\n",
    "times = times_temp    \n",
    "\n",
    "for i,x in enumerate(sh_list):\n",
    "    utt = corpus.get_utterance(x[0])\n",
    "    posted_time = datetime.fromtimestamp(utt.timestamp)\n",
    "    y = 0\n",
    "    while (posted_time > times[y]):\n",
    "        y = y + 1\n",
    "    ## this gives us the timeframe to mark it as\n",
    "    num_posts_sh[y-1] = num_posts_sh[y-1] + 1\n",
    "print(num_posts_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bin_times,num_posts_sh)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Number of posts')\n",
    "plt.title('Number of posts per day 7 days after Sandy Hook')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "## function takes in utterance and counts total words\n",
    "def count_total_words(utt):\n",
    "    if utt.text != None:\n",
    "        tokenized = word_tokenize(utt.text.lower())\n",
    "        return len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check above function works as expected\n",
    "print(count_total_words(corpus.get_utterance(utter_ids[400080])))\n",
    "print(corpus.get_utterance(utter_ids[400080]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## next, I want the incidence rate per day of political words\n",
    "\n",
    "inc_rate_sh = [0]*ps\n",
    "total_pol_words_sh = [0]*ps\n",
    "total_words_sh = [0]*ps\n",
    "\n",
    "for i,x in enumerate(sh_list):\n",
    "    utt = corpus.get_utterance(x[0])\n",
    "    posted_time = datetime.fromtimestamp(utt.timestamp)\n",
    "    y = 0\n",
    "    while (posted_time > times[y]):\n",
    "        \n",
    "        y = y + 1\n",
    "               \n",
    "    ## this gives us the timeframe to mark it as\n",
    "    total_pol_words_sh[y-1] = total_pol_words_sh[y-1] + utt.meta['num_pol_refs']\n",
    "    total_words_sh[y-1] = total_words_sh[y-1] + count_total_words(utt)\n",
    "    if y == 0:\n",
    "        print(count_total_words(utt))\n",
    "\n",
    "print(total_pol_words_sh)\n",
    "print(total_words_sh)\n",
    "\n",
    "for i in range(0, ps):\n",
    "    if total_words_sh[i] != 0:\n",
    "        inc_rate_sh[i] = total_pol_words_sh[i]/total_words_sh[i]\n",
    "    \n",
    "print(inc_rate_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(bin_times,inc_rate_sh)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.ylabel('Political words/total words')\n",
    "plt.title('Incidence rate of political words in utterances')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
