{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides examples of how to use convokit to perform analyses of behaviors of particular users within conversations. In other words, we will be dealing with attributes at the (user, conversation) level.\n",
    "Attributes at this granularity include linguistic diversity, described in the following paper : http://www.cs.cornell.edu/~cristian/Finding_your_voice__linguistic_development.html\n",
    "They can be used to perform longitudinal analyses of user behaviors across multiple conversations.\n",
    "\n",
    "Since we cannot publicly release the dataset of counseling conversations used in that paper, we will use the ChangeMyView subreddit as a test case---as such, this notebook is mostly to demonstrate how the functionality works, rather than to suggest any substantive scientific claims about longitudinal behavior change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and loading corpora:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import convokit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at /home/justine/.convokit/downloads/subreddit-changemyview\n"
     ]
    }
   ],
   "source": [
    "### comment out to download the corpus\n",
    "filename = convokit.download('subreddit-changemyview')\n",
    "\n",
    "### modify path to load your own copy\n",
    "# filename = '~/.convokit/downloads/subreddit-changemyview/'\n",
    "corpus = convokit.Corpus(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 217100\n",
      "Number of Utterances: 5017556\n",
      "Number of Conversations: 117492\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we will set up a data structure mapping each user to their conversations, and each utterance they contributed in the conversation.\n",
    "\n",
    "To do this we run the `UserConvoHistory` transformer, which annotates each `User` in a corpus with a dict of conversations --> the user's utterances in that conversation, and the timestamp of their first utterance (i.e., when they \"entered\" the conversation).\n",
    "\n",
    "Note that we can specify what counts as participating in a conversation. Here, we omit posts and focus only on comments (such that a user doesn't count as participating if they only submitted the root post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also remove moderators and deleted users\n",
    "USER_BLACKLIST = ['[deleted]', 'DeltaBot','AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return (utterance.id != utterance.root) and (utterance.user.name not in USER_BLACKLIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uchistory = convokit.user_convo_helpers.user_convo_history.UserConvoHistory(utterance_filter=utterance_is_valid)\n",
    "corpus = uchistory.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of user-level annotation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'n_utterances': 1,\n",
       " 'start_time': 1424463398,\n",
       " 'utterance_ids': ['cort13k']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['conversations']['2wkciy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1039"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['n_convos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1424463398"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.get_user('ThatBelligerentSloth').meta['start_time']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "to speed up this demo, we will only take the top 100 most active users. to help with this we will call the `get_user_convo_attribute_table` function, explained later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_convo_df = convokit.user_convo_helpers.user_convo_utils.get_user_convo_attribute_table(corpus, [])\n",
    "top_users = user_convo_df.groupby('user').size().sort_values(ascending=False).head(100).index\n",
    "subset_utts = []\n",
    "for user in top_users:\n",
    "    subset_utts += list(corpus.get_user(user).iter_utterances())\n",
    "subset_corpus = convokit.Corpus(utterances=subset_utts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Users: 100\n",
      "Number of Utterances: 539413\n",
      "Number of Conversations: 66051\n"
     ]
    }
   ],
   "source": [
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finally, to finish setting things up we will tokenize the utterances (this takes a while), using the `Tokenizer` transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 utterances tokenized\n",
      "10000 utterances tokenized\n",
      "15000 utterances tokenized\n",
      "20000 utterances tokenized\n",
      "25000 utterances tokenized\n",
      "30000 utterances tokenized\n",
      "35000 utterances tokenized\n",
      "40000 utterances tokenized\n",
      "45000 utterances tokenized\n",
      "50000 utterances tokenized\n",
      "55000 utterances tokenized\n",
      "60000 utterances tokenized\n",
      "65000 utterances tokenized\n",
      "70000 utterances tokenized\n",
      "75000 utterances tokenized\n",
      "80000 utterances tokenized\n",
      "85000 utterances tokenized\n",
      "90000 utterances tokenized\n",
      "95000 utterances tokenized\n",
      "100000 utterances tokenized\n",
      "105000 utterances tokenized\n",
      "110000 utterances tokenized\n",
      "115000 utterances tokenized\n",
      "120000 utterances tokenized\n",
      "125000 utterances tokenized\n",
      "130000 utterances tokenized\n",
      "135000 utterances tokenized\n",
      "140000 utterances tokenized\n",
      "145000 utterances tokenized\n",
      "150000 utterances tokenized\n",
      "155000 utterances tokenized\n",
      "160000 utterances tokenized\n",
      "165000 utterances tokenized\n",
      "170000 utterances tokenized\n",
      "175000 utterances tokenized\n",
      "180000 utterances tokenized\n",
      "185000 utterances tokenized\n",
      "190000 utterances tokenized\n",
      "195000 utterances tokenized\n",
      "200000 utterances tokenized\n",
      "205000 utterances tokenized\n",
      "210000 utterances tokenized\n",
      "215000 utterances tokenized\n",
      "220000 utterances tokenized\n",
      "225000 utterances tokenized\n",
      "230000 utterances tokenized\n",
      "235000 utterances tokenized\n",
      "240000 utterances tokenized\n",
      "245000 utterances tokenized\n",
      "250000 utterances tokenized\n",
      "255000 utterances tokenized\n",
      "260000 utterances tokenized\n",
      "265000 utterances tokenized\n",
      "270000 utterances tokenized\n",
      "275000 utterances tokenized\n",
      "280000 utterances tokenized\n",
      "285000 utterances tokenized\n",
      "290000 utterances tokenized\n",
      "295000 utterances tokenized\n",
      "300000 utterances tokenized\n",
      "305000 utterances tokenized\n",
      "310000 utterances tokenized\n",
      "315000 utterances tokenized\n",
      "320000 utterances tokenized\n",
      "325000 utterances tokenized\n",
      "330000 utterances tokenized\n",
      "335000 utterances tokenized\n",
      "340000 utterances tokenized\n",
      "345000 utterances tokenized\n",
      "350000 utterances tokenized\n",
      "355000 utterances tokenized\n",
      "360000 utterances tokenized\n",
      "365000 utterances tokenized\n",
      "370000 utterances tokenized\n",
      "375000 utterances tokenized\n",
      "380000 utterances tokenized\n",
      "385000 utterances tokenized\n",
      "390000 utterances tokenized\n",
      "395000 utterances tokenized\n",
      "400000 utterances tokenized\n",
      "405000 utterances tokenized\n",
      "410000 utterances tokenized\n",
      "415000 utterances tokenized\n",
      "420000 utterances tokenized\n",
      "425000 utterances tokenized\n",
      "430000 utterances tokenized\n",
      "435000 utterances tokenized\n",
      "440000 utterances tokenized\n",
      "445000 utterances tokenized\n",
      "450000 utterances tokenized\n",
      "455000 utterances tokenized\n",
      "460000 utterances tokenized\n",
      "465000 utterances tokenized\n",
      "470000 utterances tokenized\n",
      "475000 utterances tokenized\n",
      "480000 utterances tokenized\n",
      "485000 utterances tokenized\n",
      "490000 utterances tokenized\n",
      "495000 utterances tokenized\n",
      "500000 utterances tokenized\n",
      "505000 utterances tokenized\n",
      "510000 utterances tokenized\n",
      "515000 utterances tokenized\n",
      "520000 utterances tokenized\n",
      "525000 utterances tokenized\n",
      "530000 utterances tokenized\n",
      "535000 utterances tokenized\n"
     ]
    }
   ],
   "source": [
    "### comment out to tokenize the corpus.\n",
    "tokenizer = convokit.Tokenizer(verbosity=5000)\n",
    "subset_corpus = tokenizer.fit_transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this analysis is to examine how a user's conversational behavior looks like within a single conversation, and then how it evolves over the conversations they take. To demonstrate what this looks like we'll start with a simple attribute, wordcount. \n",
    "First, we count the words in each utterance using the `WordCount` transformer. Note this computes _per utterance_ statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordcount = convokit.WordCount()\n",
    "subset_corpus = wordcount.fit_transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we aggregate per-utterance statistics over all the utterances a particular user contributed in a conversation. That is, we will turn wordcount into a user,convo-level attribute.\n",
    "\n",
    "We call the `UserConvoAttrs` transformer to do this. Here, `agg_fn=np.mean` means that the user,convo-level attribute is an _average_ over utterance lengths, but you could replace this with your own aggregation function (e.g., `max`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "uc_wordcount = convokit.user_convo_helpers.user_convo_attrs.UserConvoAttrs('wordcount', agg_fn=np.mean)\n",
    "subset_corpus = uc_wordcount.fit_transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformer annotates each conversation in each User object with a wordcount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 0,\n",
       " 'n_utterances': 1,\n",
       " 'start_time': 1424463398,\n",
       " 'utterance_ids': ['cort13k'],\n",
       " 'wordcount': 100.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_corpus.get_user('ThatBelligerentSloth').meta['conversations']['2wkciy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use this aggregate statistic to analyze how users change behavior over time. The particular question here is whether or not users systematically increase or decrease in wordcount, and in the number of utterances contributed to each conversation.\n",
    "\n",
    "To facilitate further analyses, we'll load all the user,convo information pertaining to the attributes we want into a dataframe. We'll use the `get_user_convo_attribute_table` function to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>convo_id</th>\n",
       "      <th>convo_idx</th>\n",
       "      <th>n_utterances</th>\n",
       "      <th>user</th>\n",
       "      <th>user_n_convos</th>\n",
       "      <th>user_start_time</th>\n",
       "      <th>wordcount</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cdb03b__18x6j5</th>\n",
       "      <td>18x6j5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>cdb03b</td>\n",
       "      <td>7159</td>\n",
       "      <td>1361407894</td>\n",
       "      <td>23.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdb03b__1adg1v</th>\n",
       "      <td>1adg1v</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>cdb03b</td>\n",
       "      <td>7159</td>\n",
       "      <td>1361407894</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdb03b__1cciah</th>\n",
       "      <td>1cciah</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>cdb03b</td>\n",
       "      <td>7159</td>\n",
       "      <td>1361407894</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdb03b__1ccvs4</th>\n",
       "      <td>1ccvs4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>cdb03b</td>\n",
       "      <td>7159</td>\n",
       "      <td>1361407894</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cdb03b__1e2r7u</th>\n",
       "      <td>1e2r7u</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>cdb03b</td>\n",
       "      <td>7159</td>\n",
       "      <td>1361407894</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               convo_id  convo_idx  n_utterances    user  user_n_convos  \\\n",
       "key                                                                       \n",
       "cdb03b__18x6j5   18x6j5          0             2  cdb03b           7159   \n",
       "cdb03b__1adg1v   1adg1v          1             1  cdb03b           7159   \n",
       "cdb03b__1cciah   1cciah          2             2  cdb03b           7159   \n",
       "cdb03b__1ccvs4   1ccvs4          3             1  cdb03b           7159   \n",
       "cdb03b__1e2r7u   1e2r7u          4             1  cdb03b           7159   \n",
       "\n",
       "                user_start_time  wordcount  \n",
       "key                                         \n",
       "cdb03b__18x6j5       1361407894       23.5  \n",
       "cdb03b__1adg1v       1361407894        4.0  \n",
       "cdb03b__1cciah       1361407894       25.0  \n",
       "cdb03b__1ccvs4       1361407894       33.0  \n",
       "cdb03b__1e2r7u       1361407894       74.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_convo_len_df = convokit.user_convo_helpers.user_convo_utils.get_user_convo_attribute_table(subset_corpus, ['wordcount', 'n_utterances'])\n",
    "user_convo_len_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform our longitudinal analyses at the level of life-stages: i.e., contiguous blocks of conversations. Here, we compare between the first two life-stages of 10 conversations: how the user behaves in their first 10, versus their 10th to 20th conversations. \n",
    "We say that users systematically increase (or decrease) in an attribute if for a significant majority of users the value of this attribute at one life-stage increases to the next. \n",
    "\n",
    "To this end, we need to aggregate attributes over a life-stage, e.g., mean wordcount. To perform this aggregation we'll use the `get_lifestage_attributes` function, specifying lifestages of 10 conversations each, with a max of 20 conversations (i.e., 2 life-stages). Note that this function omits all users with less than 20 conversations---so we are not biased by survivorship.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>convo_idx</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACrusaderA</th>\n",
       "      <td>98.522045</td>\n",
       "      <td>130.283333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_Mirror</th>\n",
       "      <td>70.150000</td>\n",
       "      <td>93.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A_Soporific</th>\n",
       "      <td>286.900000</td>\n",
       "      <td>228.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AlphaGoGoDancer</th>\n",
       "      <td>291.550000</td>\n",
       "      <td>410.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Amablue</th>\n",
       "      <td>160.000000</td>\n",
       "      <td>295.716667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "convo_idx                 0           1\n",
       "user                                   \n",
       "ACrusaderA        98.522045  130.283333\n",
       "A_Mirror          70.150000   93.183333\n",
       "A_Soporific      286.900000  228.166667\n",
       "AlphaGoGoDancer  291.550000  410.733333\n",
       "Amablue          160.000000  295.716667"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_wc_df = convokit.user_convo_helpers.user_convo_utils.get_lifestage_attributes(user_convo_len_df, 'wordcount', 10, 20)\n",
    "stage_wc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convo_idx\n",
       "0    155.192666\n",
       "1    145.403478\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_wc_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just looking at the means, it looks like there's a slight decrease in wordcount across the population from the first to the second lifestage. To check significance, we can compute that % of users who experience this decrease, and see if it's significant per a binomial test against a null proportion of 50% of users (ie., people randomly increase or decrease)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_lifestage_comparisons(stage_df):\n",
    "    for i in range(stage_df.columns.max()):\n",
    "        \n",
    "        mask = stage_df[i+1].notnull() & stage_df[i].notnull()\n",
    "        c1 = stage_df[i+1][mask]\n",
    "        c0 = stage_df[i][mask]\n",
    "        \n",
    "        print('stages %d vs %d (%d users)' % (i + 1, i, sum(mask)))\n",
    "        n_more = sum(c1 > c0)\n",
    "        n = sum(c1 != c0)\n",
    "        print('\\tprop more: %.3f, binom_p=%.2f' % (n_more/n, stats.binom_test(n_more,n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that this is (almost) significant ... maybe more data would help!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stages 1 vs 0 (100 users)\n",
      "\tprop more: 0.410, binom_p=0.09\n"
     ]
    }
   ],
   "source": [
    "print_lifestage_comparisons(stage_wc_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the analogous analyses for number of utterances contributed suggests that users don't systematically increase or decrease between their first two life-stages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stage_convo_len_df = convokit.user_convo_helpers.user_convo_utils.get_lifestage_attributes(user_convo_len_df, 'n_utterances', 10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "convo_idx\n",
       "0    2.786\n",
       "1    2.733\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stage_convo_len_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stages 1 vs 0 (100 users)\n",
      "\tprop more: 0.458, binom_p=0.48\n"
     ]
    }
   ],
   "source": [
    "print_lifestage_comparisons(stage_convo_len_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll compute some attributes related to linguistic diversity described in the following paper : http://www.cs.cornell.edu/~cristian/Finding_your_voice__linguistic_development.html \n",
    "\n",
    "In short, for each life-stage, we compare the words used by one user in one conversation to the words they use in their other conversations, or the words that others use. As such, this is a user,convo-level attribute. Given our small sample here (and the fact that CMV and crisis counseling conversations are very different), we're not going for any scientific claims, but use the following function calls to demostrate how the pipeline would work.\n",
    "\n",
    "These attributes are all computed through the `UserConvoDiversity` transformer, which computes three attributes:\n",
    "\n",
    "* `self_div`: within-diversity in the paper, comparing language use across a user's own conversations\n",
    "* `other_div`: between-diversity in the paper, comparing language use across different users\n",
    "* `adj_other_div`: relative diversity: between - within. (intuitively, is the diversity coming from users being different from others, beyond being diverse in their own right?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = convokit.UserConvoDiversity(10, 20, n_iters=10, test=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(this takes a while to run, especially with more users involved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing corpus\n",
      "computing diversities\n",
      "250 / 439\n"
     ]
    }
   ],
   "source": [
    "subset_corpus = dt.fit_transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "div_df = convokit.user_convo_helpers.user_convo_utils.get_user_convo_attribute_table(subset_corpus, ['n_utterances','self_div','other_div',\n",
    "                                             'adj_other_div','tokens','wordcount'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note that one present limitation of this methodology is that it requires a user's activity in a conversation---and in their other conversations---to be substantive enough. if a user doesn't meet the minimum wordcount per conversation, then the function returns `np.nan` for that particular user,conversation. Filtering out these null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(366, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_df = div_df[div_df.self_div.notnull()]\n",
    "div_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as with the wordcount example, we can make cross-lifestage comparisons. here we see that there _might_ be some effect with relative diversity---users get more diverse. This might be worth exploring with more users, though note that interpreting this result for CMV versus for counseling conversations where users are randomly assigned might be different. Here, users may appear more diverse because they self-select more esoteric challengers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self_div\n",
      "stages 1 vs 0 (52 users)\n",
      "\tprop more: 0.519, binom_p=0.89\n",
      "\n",
      "\n",
      "===\n",
      "other_div\n",
      "stages 1 vs 0 (49 users)\n",
      "\tprop more: 0.510, binom_p=1.00\n",
      "\n",
      "\n",
      "===\n",
      "adj_other_div\n",
      "stages 1 vs 0 (49 users)\n",
      "\tprop more: 0.612, binom_p=0.15\n",
      "\n",
      "\n",
      "===\n"
     ]
    }
   ],
   "source": [
    "for attr in ['self_div', 'other_div', 'adj_other_div']:\n",
    "    print(attr)\n",
    "    stage_df = convokit.user_convo_helpers.user_convo_utils.get_lifestage_attributes(div_df, attr, 10, 20)\n",
    "    print_lifestage_comparisons(stage_df)\n",
    "    print('\\n\\n===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
